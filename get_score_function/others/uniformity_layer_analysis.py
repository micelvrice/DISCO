"""
Enhanced layer analysis using uniformity-based measures for CAKE optimization.

This module replaces the traditional PCA-based approach with uniformity measures
that better align with the core principle: uniform distributions contain more information.
"""

import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
import warnings
import argparse
import random
import json
import os
from typing import List, Tuple, Dict
from uniformity_based_optimization import (
    UniformityMeasures, 
    UniformityBasedLayerBudgetAllocator,
    CorrelatedUniformityOptimizer
)

def calculate_uniformity_based_layer_scores(past_key_values: List[Tuple[torch.Tensor, torch.Tensor]], 
                                          method: str = 'composite') -> List[float]:
    """
    Calculate layer importance scores based on distribution uniformity.
    
    Args:
        past_key_values: List of (key, value) tensors for each layer
        method: Uniformity measurement method
        
    Returns:
        Layer uniformity scores (higher = more uniform = more important)
    """
    results = []
    
    for layer_idx, (key_states, value_states) in enumerate(past_key_values):
        # Extract key and value from tensor format
        if isinstance(key_states, tuple):
            key_tensor = key_states[0]
            value_tensor = key_states[1]
        else:
            key_tensor = key_states
            value_tensor = value_states
            
        # Ensure correct tensor format [batch, num_heads, seq_len, head_dim]
        if len(key_tensor.shape) == 4:
            key_data = key_tensor.squeeze(0) if key_tensor.shape[0] == 1 else key_tensor[0]
            value_data = value_tensor.squeeze(0) if value_tensor.shape[0] == 1 else value_tensor[0]
        else:
            key_data = key_tensor
            value_data = value_tensor
            
        # Calculate activation patterns
        key_activations = torch.norm(key_data, dim=-1)  # [num_heads, seq_len]
        value_activations = torch.norm(value_data, dim=-1)
        
        # Combine activations across heads and sequence
        combined_activations = torch.cat([
            key_activations.flatten(),
            value_activations.flatten()
        ])
        
        # Ensure positive values for uniformity calculation
        combined_activations = torch.abs(combined_activations) + 1e-8
        
        # Calculate uniformity score based on chosen method
        if method == 'entropy':
            uniformity_score = UniformityMeasures.entropy_uniformity(
                combined_activations.unsqueeze(0)
            ).item()
        elif method == 'gini':
            uniformity_score = UniformityMeasures.gini_uniformity(
                combined_activations.unsqueeze(0)
            ).item()
        elif method == 'kl_divergence':
            uniformity_score = UniformityMeasures.kl_divergence_from_uniform(
                combined_activations.unsqueeze(0)
            ).item()
        elif method == 'composite':
            uniformity_score = UniformityMeasures.composite_uniformity(
                combined_activations.unsqueeze(0)
            ).item()
        else:
            # Fallback to entropy
            uniformity_score = UniformityMeasures.entropy_uniformity(
                combined_activations.unsqueeze(0)
            ).item()
        
        results.append(uniformity_score)
    
    return results

def build_chat(tokenizer, prompt, model_name):
    """Build chat prompt format for different models."""
    if "llama3" in model_name:
        print("======== llama3 build chat ========")
        prompt = f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
    elif "llama2" in model_name:
        prompt = f"[INST]{prompt}[/INST]"
    elif "mistral" in model_name:
        print("======== mistral build chat ========")
        prompt = f'<s>[INST] {prompt} [/INST]'
    elif "qwen" in model_name:
        print("======== qwen build chat ========")
        messages = [
            {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ]
        prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    return prompt

def analyze_uniformity_patterns(results_dict: Dict, save_path: str = None) -> Dict:
    """
    Analyze uniformity patterns across different datasets and conditions.
    
    Args:
        results_dict: Dictionary containing uniformity scores for different conditions
        save_path: Optional path to save analysis results
        
    Returns:
        Analysis summary dictionary
    """
    analysis = {
        'cross_dataset_consistency': {},
        'layer_ranking_stability': {},
        'uniformity_distribution_stats': {},
        'optimal_budget_recommendations': {}
    }
    
    # Cross-dataset consistency analysis
    if 'datasets' in results_dict:
        dataset_scores = results_dict['datasets']
        dataset_names = list(dataset_scores.keys())
        
        if len(dataset_names) > 1:
            # Calculate correlation between datasets
            correlations = {}
            for i, dataset1 in enumerate(dataset_names):
                for j, dataset2 in enumerate(dataset_names[i+1:], i+1):
                    scores1 = np.array(dataset_scores[dataset1])
                    scores2 = np.array(dataset_scores[dataset2])
                    correlation = np.corrcoef(scores1, scores2)[0, 1]
                    correlations[f"{dataset1}_vs_{dataset2}"] = correlation
            
            analysis['cross_dataset_consistency'] = {
                'correlations': correlations,
                'mean_correlation': np.mean(list(correlations.values())),
                'consistency_score': 'High' if np.mean(list(correlations.values())) > 0.8 else 'Medium' if np.mean(list(correlations.values())) > 0.6 else 'Low'
            }
    
    # Layer ranking stability
    if 'uniformity_methods' in results_dict:
        method_scores = results_dict['uniformity_methods']
        
        rankings = {}
        for method, scores in method_scores.items():
            rankings[method] = np.argsort(scores)[::-1]  # Descending order
        
        if len(rankings) > 1:
            # Calculate rank correlation between methods
            method_names = list(rankings.keys())
            rank_correlations = {}
            
            for i, method1 in enumerate(method_names):
                for j, method2 in enumerate(method_names[i+1:], i+1):
                    from scipy.stats import spearmanr
                    correlation, _ = spearmanr(rankings[method1], rankings[method2])
                    rank_correlations[f"{method1}_vs_{method2}"] = correlation
            
            analysis['layer_ranking_stability'] = {
                'rank_correlations': rank_correlations,
                'mean_rank_correlation': np.mean(list(rank_correlations.values())),
                'stability_score': 'High' if np.mean(list(rank_correlations.values())) > 0.8 else 'Medium' if np.mean(list(rank_correlations.values())) > 0.6 else 'Low'
            }
    
    # Uniformity distribution statistics
    if 'composite_scores' in results_dict:
        scores = np.array(results_dict['composite_scores'])
        
        analysis['uniformity_distribution_stats'] = {
            'mean': float(scores.mean()),
            'std': float(scores.std()),
            'min': float(scores.min()),
            'max': float(scores.max()),
            'skewness': float(((scores - scores.mean()) ** 3).mean() / (scores.std() ** 3)),
            'range': float(scores.max() - scores.min()),
            'coefficient_of_variation': float(scores.std() / scores.mean()) if scores.mean() != 0 else 0
        }
    
    # Optimal budget recommendations
    if 'composite_scores' in results_dict:
        scores = np.array(results_dict['composite_scores'])
        
        # Simulate budget allocations for different total budgets
        total_budgets = [512, 1024, 2048, 4096]
        allocator = UniformityBasedLayerBudgetAllocator(num_layers=len(scores))
        
        recommendations = {}
        for total_budget in total_budgets:
            try:
                allocation = allocator.allocate_layer_budgets(
                    total_budget, 
                    torch.tensor(scores),
                    min_budget_per_layer=max(1, total_budget // (len(scores) * 4))
                )
                
                recommendations[total_budget] = {
                    'allocation': allocation,
                    'top_5_layers': np.argsort(allocation)[-5:].tolist(),
                    'bottom_5_layers': np.argsort(allocation)[:5].tolist(),
                    'max_allocation': max(allocation),
                    'min_allocation': min(allocation),
                    'allocation_ratio': max(allocation) / max(min(allocation), 1)
                }
            except Exception as e:
                recommendations[total_budget] = {'error': str(e)}
        
        analysis['optimal_budget_recommendations'] = recommendations
    
    if save_path:
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        with open(save_path, 'w') as f:
            json.dump(analysis, f, indent=2)
    
    return analysis

def parse_args(args=None):
    parser = argparse.ArgumentParser()
    parser.add_argument('--model', type=str, default="llama3.1-8b-128k", 
                        choices=["Llama3-8b-Instruct-8k",  
                                 "llama3.1-8b-128k", 
                                 "Mistral-7b-Instruct-v0.3-32k"])
    parser.add_argument('--device', type=int, default=0)
    parser.add_argument('--dataset', type=str, default="qasper")
    parser.add_argument('--uniformity_methods', nargs='+', default=['entropy', 'gini', 'kl_divergence', 'composite'])
    parser.add_argument('--num_samples', type=int, default=20)
    parser.add_argument('--output_dir', type=str, default="uniformity_analysis")
    parser.add_argument('--compare_with_pca', action='store_true', help="Compare with original PCA method")
    parser.add_argument('--save_detailed_analysis', action='store_true', help="Save detailed analysis results")
    return parser.parse_args(args)

def seed_everything(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True
    torch.cuda.manual_seed_all(seed)

def main():
    args = parse_args()
    seed_everything(42)
    
    print(f"Running uniformity-based layer analysis for {args.model}")
    print(f"Methods: {args.uniformity_methods}")
    print(f"Dataset: {args.dataset}")
    
    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)
    
    # Initialize results storage
    results = {
        'model': args.model,
        'dataset': args.dataset,
        'num_samples': args.num_samples,
        'methods': args.uniformity_methods,
        'uniformity_methods': {},
        'datasets': {},
        'composite_scores': []
    }
    
    # Model configuration
    model_path = {
        "Llama3-8b-Instruct-8k": "/home/sjx/kv/pre_trained_models/Llama-3-8B-Instruct-8k",
        "llama3.1-8b-128k": "/home/sjx/kv/pre_trained_models/Llama-3.1-8B-Instruct",
        "Mistral-7b-Instruct-v0.3-32k": "/home/sjx/kv/pre_trained_models/Mistral-7b-Instruct-v0.3-32k"
    }
    
    model_path_str = model_path.get(args.model, "/home/sjx/kv/pre_trained_models/Llama-3.1-8B-Instruct")
    
    # For demonstration, we'll simulate the analysis with mock data
    # In a real implementation, you would load the actual model and process data
    print("Simulating uniformity analysis with synthetic data...")
    
    # Simulate layer uniformity scores for different methods
    num_layers = 32
    
    for method in args.uniformity_methods:
        print(f"Analyzing with {method} uniformity method...")
        
        # Simulate uniformity scores (in real implementation, process actual model)
        if method == 'entropy':
            # Entropy tends to favor middle layers
            base_scores = np.random.normal(0.6, 0.1, num_layers)
            base_scores[num_layers//4:3*num_layers//4] += 0.2
        elif method == 'gini':
            # Gini coefficient might show different patterns
            base_scores = np.random.normal(0.5, 0.15, num_layers)
            base_scores[:num_layers//3] += 0.15  # Early layers more uniform
        elif method == 'kl_divergence':
            # KL divergence from uniform distribution
            base_scores = np.random.normal(0.4, 0.1, num_layers)
            base_scores[-num_layers//3:] += 0.2  # Later layers more uniform
        else:  # composite
            # Composite method balances all measures
            base_scores = np.random.normal(0.55, 0.08, num_layers)
            # Add slight layer position bias
            layer_positions = np.arange(num_layers) / num_layers
            position_bias = 0.1 * np.sin(layer_positions * np.pi)
            base_scores += position_bias
        
        # Ensure positive scores and add some noise
        method_scores = np.maximum(base_scores + np.random.normal(0, 0.02, num_layers), 0.1)
        results['uniformity_methods'][method] = method_scores.tolist()
        
        if method == 'composite':
            results['composite_scores'] = method_scores.tolist()
    
    # Simulate cross-dataset analysis
    datasets_to_test = ['qasper', 'narrativeqa', 'gov_report', 'multi_news']
    for dataset in datasets_to_test:
        print(f"Simulating analysis for dataset: {dataset}")
        
        # Each dataset might show slightly different patterns
        base_scores = np.array(results['composite_scores'])
        dataset_variation = np.random.normal(0, 0.05, num_layers)
        dataset_scores = base_scores + dataset_variation
        dataset_scores = np.maximum(dataset_scores, 0.1)
        
        results['datasets'][dataset] = dataset_scores.tolist()
    
    # Perform detailed analysis
    print("Performing detailed uniformity pattern analysis...")
    analysis = analyze_uniformity_patterns(
        results, 
        save_path=os.path.join(args.output_dir, "uniformity_analysis.json") if args.save_detailed_analysis else None
    )
    
    # Save results
    results_path = os.path.join(args.output_dir, f"uniformity_scores_{args.model}_{args.dataset}.json")
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nUniformity Analysis Results:")
    print(f"Results saved to: {results_path}")
    
    if args.save_detailed_analysis:
        analysis_path = os.path.join(args.output_dir, "uniformity_analysis.json")
        print(f"Detailed analysis saved to: {analysis_path}")
    
    # Print summary
    print(f"\nSummary for {args.model} on {args.dataset}:")
    print(f"Number of layers analyzed: {num_layers}")
    print(f"Methods tested: {', '.join(args.uniformity_methods)}")
    
    if 'composite_scores' in results:
        scores = np.array(results['composite_scores'])
        print(f"Composite uniformity scores:")
        print(f"  Mean: {scores.mean():.4f}")
        print(f"  Std:  {scores.std():.4f}")
        print(f"  Range: [{scores.min():.4f}, {scores.max():.4f}]")
        print(f"  Top 5 layers (most uniform): {np.argsort(scores)[-5:]}")
        print(f"  Bottom 5 layers (least uniform): {np.argsort(scores)[:5]}")
    
    if 'cross_dataset_consistency' in analysis:
        consistency = analysis['cross_dataset_consistency']
        if 'mean_correlation' in consistency:
            print(f"Cross-dataset consistency: {consistency['consistency_score']} "
                  f"(mean correlation: {consistency['mean_correlation']:.3f})")
    
    if 'layer_ranking_stability' in analysis:
        stability = analysis['layer_ranking_stability']
        if 'mean_rank_correlation' in stability:
            print(f"Method ranking stability: {stability['stability_score']} "
                  f"(mean rank correlation: {stability['mean_rank_correlation']:.3f})")
    
    print(f"\nâœ“ Uniformity-based layer analysis completed successfully!")

if __name__ == "__main__":
    main()